Requisites for any valid KPI (Key Performance Indicator):
===================================================

The measured data:
=> must be consistent (repeatable in time) 
=> and must be periodic (allows number of units in proportion to a unit or time)

The watched process:
=> must be a deterministic one (results must be reproducible with certain parameters) 
=> and must allow statistical analysis (high production volumes, not single-piece production)

KPI requires ample data. And, of course, the data must be accurate and realistic.
The data collection and calculations must be identical between locations.
The data must be available to all stakeholders. 
All the terms and data must be accurately defined and understood by all.



Requisitos para un indicador válido:
====================================

Los datos a medir:
=> han de ser consistentes(reproducibles a lo largo del tiempo) 
=> y han de ser periodicos (se han de poder medir en forma de unidades por unidad de tiempo)

El proceso supervisado:
=> ha de ser determinista (los resultados obtenidos han de ser reproducibles, dentro de un rango de parámetros) 
=> y ha de permitir realizar un análisis estadístico (el volumen de producción ha de ser grande, no siven produciones puntuales de unos pocos ejemplares)

Un indicador fiable requiere una amplia muestra de datos. Datos que, por supuesto, han de ser precisos y realistas.
La recolección de datos se ha de hacer de igual manera en todos los puntos de toma de datos.
Los datos han de estar a disposición de cualquier interesado.
Todas las definiciones y datos han de estar definidas con precisión y todo el mundo ha de comprenderlas bien.


Indicadores en desarrollo de software: métricas DORA
======================================
https://codeclimate.com/blog/dora-metrics

**Deployment Frequency**

Deployment Frequency (DF) measures the frequency at which code is successfully deployed to a production environment. It is a measure of a team’s average throughput over a period of time, and can be used to benchmark how often an engineering team is shipping value to customers.

Engineering teams generally strive to deploy as quickly and frequently as possible, getting new features into the hands of users to improve customer retention and stay ahead of the competition. More successful DevOps teams deliver smaller deployments more frequently, rather than batching everything up into a larger release that is deployed during a fixed window. High-performing teams deploy at least once a week, while teams at the top of their game — peak performers — deploy multiple times per day.

Low performance on this metric can inform teams that they may need to improve their automated testing and validation of new code. Another area to focus on could be breaking changes down into smaller chunks, and creating smaller pull requests (PRs), or improving overall Deploy Volume.

**Mean Lead Time for Changes**

Mean Lead Time for Changes (MLTC) helps engineering leaders understand the efficiency of their development process once coding has begun. This metric measures how long it takes for a change to make it to a production environment by looking at the average time between the first commit made in a branch and when that branch is successfully running in production. It quantifies how quickly work will be delivered to customers, with the best teams able to go from commit to production in less than a day. Average teams have an MLTC of around one week. 

Deployments can be delayed for a variety of reasons, including batching up related features and ongoing incidents, and it’s important that engineering leaders have an accurate understanding of how long it takes their team to get changes into production.

When trying to improve on this metric, leaders can analyze metrics corresponding to the stages of their development pipeline, like Time to Open, Time to First Review, and Time to Merge, to identify bottlenecks in their processes. 

Teams looking to improve on this metric might consider breaking work into smaller chunks to reduce the size of PRs, boosting the efficiency of their code review process, or investing in automated testing and deployment processes. 

**Change Failure Rate**

The Change Failure Rate (CFR) is a calculation of the percentage of deployments causing a failure in production, and is found by dividing the number of incidents by the total number of deployments. This gives leaders insight into the quality of code being shipped and by extension, the amount of time the team spends fixing failures. Most DevOps teams can achieve a change failure rate between 0% and 15%.

When changes are being frequently deployed to production environments, bugs are all but inevitable. Sometimes these bugs are minor, but in some cases these can lead to major failures. It’s important to bear in mind that these shouldn’t be used as an occasion to place blame on a single person or team; however, it’s also vital that engineering leaders monitor how often these incidents happen.

This metric is an important counterpoint to the DF and MLTC metrics. Your team may be moving quickly, but you also want to ensure they’re delivering quality code — both stability and throughput are important to successful, high-performing DevOps teams.

To improve in this area, teams can look at reducing the work-in-progress (WIP) in their iterations, boosting the efficacy of their code review processes, or investing in automated testing. 

**Mean Time to Recovery**

The Mean Time to Recovery (MTTR) measures the time it takes to restore a system to its usual functionality. For elite teams, this looks like being able to recover in under an hour, whereas for many teams, this is more likely to be under a day.

Failures happen, but the ability to quickly recover from a failure in a production environment is key to the success of DevOps teams. Improving MTTR requires DevOps teams to improve their observability so that failures can be identified and resolved quickly. 

Additional actions that can improve this metric are: having an action plan for responders to consult, ensuring the team understands the process for addressing failures, and improving MLTC. 



Comentarios extraidos de un artículo
====================================
 https://visualstudiomagazine.com/Articles/2024/01/16/rethinking-team-metrics.aspx?oly_enc_id=8264G5036434F6C

..//..
Percent complete is one to be avoided because it can literally never be accurate. I like to say that building software is not simply a typing exercise, and how much work is needed to be able to call a feature "done" is impossible to know, until it is done. So any numbers we throw out in terms of percent complete, assumes we know precisely how long the thing will take, which is why we often experience people being 95 percent done for days or even weeks. Velocity is a bit different. I don't think it is a bad metric per se, I think it can be misunderstood. Velocity is a quantitative measure of output and it only gives the team credit for features deemed done, which is qualitative. Establishing a pattern of how much work a team can take on and complete successfully can aid in expectations setting, but if you start making an increase in velocity the thing to strive for, well most software developers are GREAT at math. I can make velocity whatever number you need it to be, if all you're concerned about is the number. I encourage people to look at Velocity as a signal, and if that number start to vary wildly that's a good time to have a conversation about what's impacting their velocity and see if the team could use some support.

../..

A metrics mindset encompasses a couple of concepts. One, is that metrics should never be viewed in a vacuum. Echoing what I said previously, they are not the end goal, they are simply a signal of whether things are moving in the right direction. When we see a "blip" in the trend, we should be having informed conversations about what the metrics mean. That's lesson two of a metrics mindset, metrics should always be surrounded by conversation. Without context, we can take any number of conclusions away from a dashboard of metrics, and most of them will be wrong. This is why I always bristle when I am asked to build dashboards that will be used by managers to measure and judge teams in their department. They're often pasted into reports, and used to communicate progress and status, without the valuable context that tells the real story. If you want to know what the other principals of a metrics mindset are, be sure to come to my talk!

../..